{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f772df5f",
   "metadata": {},
   "source": [
    "## Problem 1: Hand-crafted Network\n",
    "### 1.1: Hand-crafted Neurons\n",
    "#### 1.1.1: Logical OR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63e0a7",
   "metadata": {},
   "source": [
    "We define the logical OR as:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{z}) = \\text{sign}(\\mathbf{z} \\cdot \\mathbf{\\mathbb{1}_{D,1}})\n",
    "$$\n",
    "\n",
    "Therefore we have that $b = 0, \\ \\mathbf{w} = \\mathbf{\\mathbb{1}_{D,1}}, \\ \\phi(x) = \\text{sign}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2503da1",
   "metadata": {},
   "source": [
    "#### 1.1.2: Masked OR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55060f95",
   "metadata": {},
   "source": [
    "We define the masked OR as:\n",
    "\n",
    "$$\n",
    "g(\\mathbf{z}; \\mathbf{c}) = \\text{sign}(\\mathbf{z} \\cdot \\mathbf{c})\n",
    "$$\n",
    "\n",
    "Therefore $b = 0, \\ \\mathbf{w} = \\mathbf{c}, \\ \\phi(x) = |\\text{sign}(x)|$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b7dc0f",
   "metadata": {},
   "source": [
    "#### 1.1.3: Perfect Match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cdcb7",
   "metadata": {},
   "source": [
    "We define the perfect match neuron as:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{z}; \\mathbf{c}) = 1 - |\\text{sign}((\\mathbf{z} - \\mathbf{c}) \\cdot \\mathbf{\\mathbb{1}_{D,1}})| \n",
    "$$\n",
    "\n",
    "Therefore $b = 0, \\ \\mathbf{w} = \\mathbf{\\mathbb{1}_{D,1}}, \\ \\phi(x) = 1 - |\\text{sign}(x)|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f1845",
   "metadata": {},
   "source": [
    "## Problem 2: Linear Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956fd660",
   "metadata": {},
   "source": [
    "Since $\\phi_l$ is the identity function we can define the activation of each layer as:\n",
    "\n",
    "$$\\mathbf{Z_l} = \\mathbf{Z_{l-1}} \\cdot \\mathbf{B_l} + \\mathbf{b_l}$$ \n",
    "\n",
    "\n",
    "Therefore the activation $\\mathbf{Z_L}$ of the last layer $L$ can be written as follows:\n",
    "\n",
    "$$ \\mathbf{Z_L} = (\\dots(\\mathbf{X} \\mathbf{B_1} + \\mathbf{b_1}) \\mathbf{B_2} + \\mathbf{b_3})\\dots) \\mathbf{B_L} + \\mathbf{b_L} $$\n",
    "\n",
    "This form can be reduced using the following pairwise reduction for some activations in layers $l$ and $l+1$ until there is only one layer left:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{Z_{l+1}} &= (\\mathbf{Z_{l-1}} \\mathbf{B_l} + \\mathbf{b_l}) \\mathbf{B_{l+1}} + \\mathbf{b_{l+1}} \\\\\n",
    "&= \\mathbf{Z_{l-1}} \\mathbf{B_l} \\mathbf{B_{l+1}} + \\mathbf{b_l} \\mathbf{B_{l+1}} + \\mathbf{b_{l+1}} \\\\\n",
    "&= \\mathbf{Z_{l-1}} \\bar{\\mathbf{B_l}} + \\bar{\\mathbf{b_{l}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The new parameters are defined as $\\bar{\\mathbf{B_l}} = \\mathbf{B_l} \\mathbf{B_{l+1}}$ and $\\bar{\\mathbf{b_l}} = \\mathbf{b_l} \\mathbf{B_{l+1}} + \\mathbf{b_{l+1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba4c3f9",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e92d441",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">The forward function is essentially the same as in the sample solution. <br>\n",
    "    The backwards path is also identical, but with one more multiplication. Performance wise the sample solution should be better. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d34c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "# ###################################\n",
    "class ReLULayer ( object ):\n",
    "    def forward (self , input ):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the inpu't\n",
    "        # basic relu implementation\n",
    "        relu = np.maximum(0, input)\n",
    "        return relu\n",
    "    def backward (self , upstream_gradient ) :\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input\n",
    "        # look for which input relu was called. if the input was <= 0, relu was not aplied, and hence derivative needs also be zero\n",
    "        # if input > 0, we need to set the derivative to 1\n",
    "        derivative = np.where(self.input > 0, 1, 0)\n",
    "        downstream_gradient = upstream_gradient * derivative\n",
    "        return downstream_gradient\n",
    "    def update (self , learning_rate ):\n",
    "        pass # ReLU is parameter - free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90928f09",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">forward is essentially the same, however, the nummerator is calculated differently in the sample solution, but it is stated that this difference does not impact the result <br>\n",
    "For the backwards path I certainly mixed some things up. However the saoftmax derivative calculation is almost correct, but it used the inputs as values instead of the predicted posteriors. Also I forgot to divide each sample by the length of the batch. Furthermore this solution here uses many for loops and should be rewritten in a vectorized manner as in the sample solution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f53dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer( object ):\n",
    "    def __init__ (self , n_classes ):\n",
    "        self.n_classes = n_classes\n",
    "    def forward (self, input ):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the softmax of the input\n",
    "        softmax = np.exp(input) / np.sum(np.exp(input))\n",
    "        return softmax\n",
    "    def backward(self , predicted_posteriors , true_labels ) :\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # (use cross - entropy loss and the chain rule for softmax ,\n",
    "        # as derived in the lecture )\n",
    "        \n",
    "        #calculate cross entropy loss derivative\n",
    "        loss_derivative = []\n",
    "        for row,label in enumerate(true_labels):\n",
    "            sample_loss = []\n",
    "            for class_idx in range(2):\n",
    "                if class_idx == label:\n",
    "                    sample_loss.append(-1/predicted_posteriors[row][class_idx])\n",
    "                else:\n",
    "                    sample_loss.append(0)\n",
    "            loss_derivative.append(sample_loss)        \n",
    "            \n",
    "        # calculate softmaxe derivative of the inputs \n",
    "        softmax_derivative = []\n",
    "        for row,label in enumerate(true_labels):\n",
    "            sample_softmax_derivative = []\n",
    "            for class_idx in range(2):\n",
    "                if class_idx == label:\n",
    "                    sample_softmax_derivative.append(predicted_posteriors[row][class_idx]-1)\n",
    "                else:\n",
    "                    sample_softmax_derivative.append(predicted_posteriors[row][class_idx])\n",
    "            softmax_derivative.append(sample_softmax_derivative)\n",
    "        # mltiply\n",
    "        softmax_derivative = np.asarray(softmax_derivative)\n",
    "        loss_derivative = np.asarray(loss_derivative)\n",
    "        # I do think i need to multiply the derivatives and I also believe that i implemented them according to the lecture.\n",
    "        # However if I multiply them, i receive a 2 x 2 matrix which is incompatible with the next gradient to compute...\n",
    "        # Either this is here is false or the LinearLayer gradient calculation\n",
    "        softmax_derivative /= len(true_labels) # @ loss_derivative\n",
    "        return softmax_derivative\n",
    "    def update(self , learning_rate ):\n",
    "        pass # softmax is parameter - free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb3ab8",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Initialization is quite similar, with more adjustments to the np.random function in the sample solution, which shouldn't make that big of a difference. <br>\n",
    "The forward path, it is practically the same. The only idfference is matmul instead of np.dot, which is however equivallent for 2D-Arrays.<br>\n",
    "The backwards path is identical to the sample solution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d0c32494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer( object ):\n",
    "    def __init__ (self , n_inputs , n_outputs ):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # randomly initialize weights and intercepts for the feature dimensions\n",
    "        self.B = np.random.normal(size=(n_inputs, n_outputs)) \n",
    "        self.b = np.random.normal(size=(n_outputs))\n",
    "    def forward(self , input ):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = np.asarray(input)\n",
    "        # compute the scalar product of input and weights\n",
    "        # ( these are the preactivations for the subsequent non - linear layer )\n",
    "        # the linear preactivation claclulation of the weights multiplied by the inputs and added the biases\n",
    "        preactivations = np.matmul(self.input, self.B) + self.b \n",
    "        return preactivations\n",
    "    def backward(self , upstream_gradient ) :\n",
    "        # compute the derivative of the weights from\n",
    "        # upstream_gradient and the stored input\n",
    "        self.grad_b = np.sum(upstream_gradient, axis=0)\n",
    "        self.grad_B = np.matmul(self.input.T, upstream_gradient)\n",
    "        # compute the downstream gradient to be passed to the preceding layer\n",
    "        downstream_gradient = np.matmul(upstream_gradient, self.B.T)\n",
    "        return downstream_gradient\n",
    "    def update(self , learning_rate ):\n",
    "        # update the weights by batch gradient descent\n",
    "        self.B = self.B - learning_rate * self.grad_B\n",
    "        self.b = self.b - learning_rate * self.grad_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89acdf2",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">The backward function is computed the same way as in the sample <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8b99a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP( object ):\n",
    "    def __init__ (self , n_features , layer_sizes ):\n",
    "        # constuct a multi - layer perceptron\n",
    "        # with ReLU activation in the hidden layers and softmax output\n",
    "        # (i.e. it predicts the posterior probability of a classification problem )\n",
    "        # n_features : number of inputs\n",
    "        # len ( layer_size ): number of layers\n",
    "        # layer_size [k]: number of neurons in layer k\n",
    "        # ( specifically : layer_sizes [ -1] is the number of classes )\n",
    "        self.n_layers = len( layer_sizes )\n",
    "        self.layers = []\n",
    "        # create interior layers ( linear + ReLU )\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append( LinearLayer(n_in , n_out ))\n",
    "            self.layers.append( ReLULayer() )\n",
    "            n_in = n_out\n",
    "        # create last linear layer + output layer\n",
    "        n_out = layer_sizes[ -1]\n",
    "        self.layers.append( LinearLayer(n_in , n_out ))\n",
    "        self.layers.append( OutputLayer( n_out ))\n",
    "    def forward (self , X):\n",
    "        # X is a mini - batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X (in case instances are images )\n",
    "        X = X.reshape( batch_size , -1)\n",
    "        # compute the forward pass\n",
    "        # ( implicitly stores internal activations for later backpropagation )\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward( result )\n",
    "        return result\n",
    "    def backward(self , predicted_posteriors , true_classes ):\n",
    "        # init result of the last layer\n",
    "        result = self.layers[-1].backward(predicted_posteriors , true_classes)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            # set the result to the according backpropagation function\n",
    "            result = layer.backward(result)\n",
    "        return result\n",
    "    def update(self , X, Y, learning_rate ):\n",
    "        posteriors = self.forward(X)\n",
    "        self.backward(posteriors,Y)\n",
    "        for layer in self.layers :\n",
    "            layer.update( learning_rate )\n",
    "    def train(self , x, y, n_epochs , batch_size , learning_rate ):\n",
    "        N = len (x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range( 2 ):\n",
    "            print (\" Epoch \", i)\n",
    "            # reorder data for every epoch\n",
    "            # (i.e. sample mini - batches without replacement )\n",
    "            permutation = np. random.permutation(N)\n",
    "            for batch in range( n_batches ):\n",
    "                # create mini - batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[ permutation [ start:start + batch_size ]]\n",
    "                y_batch = y[ permutation [ start:start + batch_size ]]\n",
    "                # perform one forward and backward pass and update network parameters\n",
    "                self.update( x_batch , y_batch , learning_rate )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b446f",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">The idea f calculating the error rate was  the same as in the sample solution. The sample solution used vectorization which is much faster and does not need the for loop that is used here. However this vcersion here is not correctly implemented as it outputs undesired results for the error rate because of the usage of np.where\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7da4e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  0\n",
      " Epoch  1\n",
      "Predicted Posteriors: [[1.93695100e-05 1.33823869e-03]\n",
      " [1.66849034e-06 6.91362725e-05]\n",
      " [7.52671716e-06 4.69552354e-04]\n",
      " ...\n",
      " [8.03818829e-06 5.01481372e-04]\n",
      " [7.52253506e-06 4.58712289e-04]\n",
      " [6.12584400e-06 3.54084504e-04]]\n",
      " error rate : 0.9995\n"
     ]
    }
   ],
   "source": [
    "# set training / test set size\n",
    "N = 2000\n",
    "# create training and test data\n",
    "X_train , Y_train = datasets.make_moons (N, noise =0.05)\n",
    "X_test , Y_test = datasets.make_moons (N, noise =0.05)\n",
    "n_features = 2\n",
    "n_classes = 2\n",
    "# standardize features to be in [ -1 , 1]\n",
    "offset = X_train.min(axis =0)\n",
    "scaling = X_train.max(axis =0) - offset\n",
    "X_train = (( X_train - offset ) / scaling - 0.5) * 2.0\n",
    "X_test = (( X_test - offset ) / scaling - 0.5) * 2.0\n",
    "# set hyperparameters ( play with these !)\n",
    "layer_sizes = [5 , 5, n_classes ]\n",
    "n_epochs = 5\n",
    "batch_size = 200\n",
    "learning_rate = 0.05\n",
    "# create network\n",
    "network = MLP( n_features , layer_sizes )\n",
    "# train\n",
    "network.train( X_train , Y_train , n_epochs , batch_size , learning_rate )\n",
    "# test\n",
    "predicted_posteriors = network.forward( X_test )\n",
    "print(f\"Predicted Posteriors: {predicted_posteriors}\")\n",
    "# determine class predictions from posteriors by winner -takes - all rule\n",
    "\n",
    "predicted_classes = [ np.argmax(sample) for sample in predicted_posteriors]\n",
    "# compute and output the error rate of predicted_classes\n",
    "error_rate = (1 - (len(np.where(Y_test == predicted_classes))) / len(Y_test))\n",
    "# compute and output the error rate of predicted_classes\n",
    "print (\" error rate :\", error_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "19ac5aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      " Epoch  0\n",
      " Epoch  1\n",
      "Predicted Posteriors: [[8.40941617e-05 4.15905838e-04]\n",
      " [8.40941617e-05 4.15905838e-04]\n",
      " [8.40941617e-05 4.15905838e-04]\n",
      " ...\n",
      " [8.40941617e-05 4.15905838e-04]\n",
      " [8.40941617e-05 4.15905838e-04]\n",
      " [8.40941617e-05 4.15905838e-04]]\n",
      " error rate : 0.9995\n",
      "--------------------------------------------------\n",
      " Epoch  0\n",
      " Epoch  1\n",
      "Predicted Posteriors: [[8.77625077e-05 5.69352206e-04]\n",
      " [6.40572368e-05 3.90528277e-04]\n",
      " [1.06201825e-04 7.15402119e-04]\n",
      " ...\n",
      " [5.40236897e-05 3.18468708e-04]\n",
      " [4.78781144e-05 2.75593125e-04]\n",
      " [4.98587099e-05 2.89298893e-04]]\n",
      " error rate : 0.9995\n",
      "--------------------------------------------------\n",
      " Epoch  0\n",
      " Epoch  1\n",
      "Predicted Posteriors: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " ...\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      " error rate : 0.9995\n",
      "--------------------------------------------------\n",
      " Epoch  0\n",
      " Epoch  1\n",
      "Predicted Posteriors: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " ...\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      " error rate : 0.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2040/2305622641.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  softmax = np.exp(input) / np.sum(np.exp(input))\n",
      "/tmp/ipykernel_2040/2305622641.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  softmax = np.exp(input) / np.sum(np.exp(input))\n",
      "/tmp/ipykernel_2040/2305622641.py:21: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  sample_loss.append(-1/predicted_posteriors[row][class_idx])\n"
     ]
    }
   ],
   "source": [
    "# test the implementation\n",
    "test_cases = [  MLP( n_features , [2 , 2 , n_classes ]),\n",
    "                MLP( n_features , [3 , 3 , n_classes ]),\n",
    "                MLP( n_features , [5 , 5 , n_classes ]),\n",
    "                MLP( n_features , [30 , 30 , n_classes ])]\n",
    "\n",
    "for test_nn in test_cases:\n",
    "    print(50*\"-\")\n",
    "    test_nn.train(X_train , Y_train , n_epochs , batch_size , learning_rate)\n",
    "    predicted_posteriors = test_nn.forward( X_test )\n",
    "    print(f\"Predicted Posteriors: {predicted_posteriors}\")\n",
    "    predicted_classes = [ np.argmax(sample) for sample in predicted_posteriors]\n",
    "    error_rate = 1 - (len(np.where(Y_test == predicted_classes)) / len(Y_test))\n",
    "    print (\" error rate :\", error_rate )\n",
    "# the test results are bad because of the wrong implementation of the backpropagation in the outputlayer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
