{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f772df5f",
   "metadata": {},
   "source": [
    "## Number 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba4c3f9",
   "metadata": {},
   "source": [
    "## Number 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "9d34c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "# ###################################\n",
    "class ReLULayer ( object ):\n",
    "    def forward (self , input ):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the inpu't\n",
    "        # basic relu implementation\n",
    "        relu = np.maximum(0, input)\n",
    "        return relu\n",
    "    def backward (self , upstream_gradient ) :\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input\n",
    "        # look for which input relu was called. if the input was <= 0, relu was not aplied, and hence derivative needs also be zero\n",
    "        # if input > 0, we need to set the derivative to 1\n",
    "        derivative = np.where(self.input > 0, 1, 0)\n",
    "        downstream_gradient = upstream_gradient * derivative\n",
    "        return downstream_gradient\n",
    "    def update (self , learning_rate ):\n",
    "        pass # ReLU is parameter - free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "3f53dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer( object ):\n",
    "    def __init__ (self , n_classes ):\n",
    "        self.n_classes = n_classes\n",
    "    def forward (self, input ):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the softmax of the input\n",
    "        softmax = np.exp(input) / np.sum(np.exp(input))\n",
    "        return softmax\n",
    "    def backward(self , predicted_posteriors , true_labels ) :\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # (use cross - entropy loss and the chain rule for softmax ,\n",
    "        # as derived in the lecture )\n",
    "        \n",
    "        #calculate cross entropy loss derivative\n",
    "        loss_derivative = []\n",
    "        for row,label in enumerate(true_labels):\n",
    "            sample_loss = []\n",
    "            for class_idx in range(2):\n",
    "                if class_idx == label:\n",
    "                    sample_loss.append(-1/predicted_posteriors[row][class_idx])\n",
    "                else:\n",
    "                    sample_loss.append(0)\n",
    "            loss_derivative.append(sample_loss)        \n",
    "            \n",
    "        # calculate softmaxe derivative of the inputs \n",
    "        softmax_derivative = []\n",
    "        for row,label in enumerate(true_labels):\n",
    "            sample_softmax_derivative = []\n",
    "            for class_idx in range(2):\n",
    "                if class_idx == label:\n",
    "                    sample_softmax_derivative.append(self.input[row][class_idx]-1)\n",
    "                else:\n",
    "                    sample_softmax_derivative.append(self.input[row][class_idx])\n",
    "            softmax_derivative.append(sample_softmax_derivative)\n",
    "        # mltiply\n",
    "        softmax_derivative = np.asarray(softmax_derivative)\n",
    "        loss_derivative = np.asarray(loss_derivative)\n",
    "        # I do think i need to multiply the derivatives and I also believe that i implemented them according to the lecture.\n",
    "        # However if I multiply them, i receive a 2 x 2 matrix which is incompatible with the next gradient to compute...\n",
    "        # Either this is here is false or the LinearLayer gradient calculation\n",
    "        return softmax_derivative # @ loss_derivative\n",
    "\n",
    "    def update(self , learning_rate ):\n",
    "        pass # softmax is parameter - free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "d0c32494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer( object ):\n",
    "    def __init__ (self , n_inputs , n_outputs ):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # randomly initialize weights and intercepts for the feature dimensions\n",
    "        self.B = np.random.normal(size=(n_inputs, n_outputs)) \n",
    "        self.b = np.random.normal(size=(n_outputs))\n",
    "    def forward(self , input ):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = np.asarray(input)\n",
    "        # compute the scalar product of input and weights\n",
    "        # ( these are the preactivations for the subsequent non - linear layer )\n",
    "        # the linear preactivation claclulation of the weights multiplied by the inputs and added the biases\n",
    "        preactivations = np.matmul(self.input, self.B) + self.b \n",
    "        return preactivations\n",
    "    def backward(self , upstream_gradient ) :\n",
    "        # compute the derivative of the weights from\n",
    "        # upstream_gradient and the stored input\n",
    "        self.grad_b = np.sum(upstream_gradient, axis=0)\n",
    "        self.grad_B = np.matmul(self.input.T, upstream_gradient)\n",
    "        # compute the downstream gradient to be passed to the preceding layer\n",
    "        downstream_gradient = np.matmul(upstream_gradient, self.B.T)\n",
    "        return downstream_gradient\n",
    "    def update(self , learning_rate ):\n",
    "        # update the weights by batch gradient descent\n",
    "        self.B = self.B - learning_rate * self.grad_B\n",
    "        self.b = self.b - learning_rate * self.grad_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "8b99a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP( object ):\n",
    "    def __init__ (self , n_features , layer_sizes ):\n",
    "        # constuct a multi - layer perceptron\n",
    "        # with ReLU activation in the hidden layers and softmax output\n",
    "        # (i.e. it predicts the posterior probability of a classification problem )\n",
    "        # n_features : number of inputs\n",
    "        # len ( layer_size ): number of layers\n",
    "        # layer_size [k]: number of neurons in layer k\n",
    "        # ( specifically : layer_sizes [ -1] is the number of classes )\n",
    "        self.n_layers = len( layer_sizes )\n",
    "        self.layers = []\n",
    "        # create interior layers ( linear + ReLU )\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append( LinearLayer(n_in , n_out ))\n",
    "            self.layers.append( ReLULayer() )\n",
    "            n_in = n_out\n",
    "        # create last linear layer + output layer\n",
    "        n_out = layer_sizes[ -1]\n",
    "        self.layers.append( LinearLayer(n_in , n_out ))\n",
    "        self.layers.append( OutputLayer( n_out ))\n",
    "    def forward (self , X):\n",
    "        # X is a mini - batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X (in case instances are images )\n",
    "        X = X.reshape( batch_size , -1)\n",
    "        # compute the forward pass\n",
    "        # ( implicitly stores internal activations for later backpropagation )\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward( result )\n",
    "        return result\n",
    "    def backward(self , predicted_posteriors , true_classes ):\n",
    "        # init result of the last layer\n",
    "        result = self.layers[-1].backward(predicted_posteriors , true_classes)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            # set the result to the according backpropagation function\n",
    "            result = layer.backward(result)\n",
    "        return result\n",
    "    def update(self , X, Y, learning_rate ):\n",
    "        posteriors = self.forward(X)\n",
    "        self.backward(posteriors,Y)\n",
    "        for layer in self.layers :\n",
    "            layer.update( learning_rate )\n",
    "    def train(self , x, y, n_epochs , batch_size , learning_rate ):\n",
    "        N = len (x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range( 2 ):\n",
    "            print (\" Epoch \", i)\n",
    "            # reorder data for every epoch\n",
    "            # (i.e. sample mini - batches without replacement )\n",
    "            permutation = np. random.permutation(N)\n",
    "            for batch in range( n_batches ):\n",
    "                # create mini - batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[ permutation [ start:start + batch_size ]]\n",
    "                y_batch = y[ permutation [ start:start + batch_size ]]\n",
    "                # perform one forward and backward pass and update network parameters\n",
    "                self.update( x_batch , y_batch , learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "7da4e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch  0\n",
      " Epoch  1\n",
      "Predicted Posteriors: [[nan  0.]\n",
      " [nan  0.]\n",
      " [nan  0.]\n",
      " ...\n",
      " [nan  0.]\n",
      " [nan  0.]\n",
      " [nan  0.]]\n",
      " error rate : 0.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_560/2424381600.py:21: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  sample_loss.append(-1/predicted_posteriors[row][class_idx])\n",
      "/tmp/ipykernel_560/2424381600.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  softmax = np.exp(input) / np.sum(np.exp(input))\n",
      "/tmp/ipykernel_560/2424381600.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  softmax = np.exp(input) / np.sum(np.exp(input))\n"
     ]
    }
   ],
   "source": [
    "# set training / test set size\n",
    "N = 2000\n",
    "# create training and test data\n",
    "X_train , Y_train = datasets.make_moons (N, noise =0.05)\n",
    "X_test , Y_test = datasets.make_moons (N, noise =0.05)\n",
    "n_features = 2\n",
    "n_classes = 2\n",
    "# standardize features to be in [ -1 , 1]\n",
    "offset = X_train.min(axis =0)\n",
    "scaling = X_train.max(axis =0) - offset\n",
    "X_train = (( X_train - offset ) / scaling - 0.5) * 2.0\n",
    "X_test = (( X_test - offset ) / scaling - 0.5) * 2.0\n",
    "# set hyperparameters ( play with these !)\n",
    "layer_sizes = [5 , 5, n_classes ]\n",
    "n_epochs = 5\n",
    "batch_size = 200\n",
    "learning_rate = 0.05\n",
    "# create network\n",
    "network = MLP( n_features , layer_sizes )\n",
    "# train\n",
    "network.train( X_train , Y_train , n_epochs , batch_size , learning_rate )\n",
    "# test\n",
    "predicted_posteriors = network.forward( X_test )\n",
    "print(f\"Predicted Posteriors: {predicted_posteriors}\")\n",
    "# determine class predictions from posteriors by winner -takes - all rule\n",
    "\n",
    "predicted_classes = [ np.argmax(sample) for sample in predicted_posteriors]\n",
    "# compute and output the error rate of predicted_classes\n",
    "error_rate = 1 - (len(np.where(Y_test == predicted_classes)) / len(Y_test))\n",
    "print (\" error rate :\", error_rate )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "19ac5aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      " Epoch  0\n",
      " Epoch  1\n",
      "Predicted Posteriors: [[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " ...\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n",
      " error rate : 0.9995\n",
      "--------------------------------------------------\n",
      " Epoch  0\n",
      " Epoch  1\n",
      "Predicted Posteriors: [[ 0. nan]\n",
      " [ 0. nan]\n",
      " [ 0. nan]\n",
      " ...\n",
      " [ 0. nan]\n",
      " [ 0. nan]\n",
      " [ 0. nan]]\n",
      " error rate : 0.9995\n",
      "--------------------------------------------------\n",
      " Epoch  0\n",
      " Epoch  1\n",
      "Predicted Posteriors: [[ 0. nan]\n",
      " [ 0. nan]\n",
      " [ 0. nan]\n",
      " ...\n",
      " [ 0. nan]\n",
      " [ 0. nan]\n",
      " [ 0. nan]]\n",
      " error rate : 0.9995\n",
      "--------------------------------------------------\n",
      " Epoch  0\n",
      " Epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_560/2424381600.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  softmax = np.exp(input) / np.sum(np.exp(input))\n",
      "/tmp/ipykernel_560/2424381600.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  softmax = np.exp(input) / np.sum(np.exp(input))\n",
      "/home/max/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/tmp/ipykernel_560/2424381600.py:21: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  sample_loss.append(-1/predicted_posteriors[row][class_idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Posteriors: [[ 0. nan]\n",
      " [ 0. nan]\n",
      " [ 0. nan]\n",
      " ...\n",
      " [ 0. nan]\n",
      " [ 0. nan]\n",
      " [ 0. nan]]\n",
      " error rate : 0.9995\n"
     ]
    }
   ],
   "source": [
    "# test the implementation\n",
    "test_cases = [  MLP( n_features , [2 , 2 , n_classes ]),\n",
    "                MLP( n_features , [3 , 3 , n_classes ]),\n",
    "                MLP( n_features , [5 , 5 , n_classes ]),\n",
    "                MLP( n_features , [30 , 30 , n_classes ])]\n",
    "\n",
    "for test_nn in test_cases:\n",
    "    print(50*\"-\")\n",
    "    test_nn.train(X_train , Y_train , n_epochs , batch_size , learning_rate)\n",
    "    predicted_posteriors = test_nn.forward( X_test )\n",
    "    print(f\"Predicted Posteriors: {predicted_posteriors}\")\n",
    "    predicted_classes = [ np.argmax(sample) for sample in predicted_posteriors]\n",
    "    error_rate = 1 - (len(np.where(Y_test == predicted_classes)) / len(Y_test))\n",
    "    print (\" error rate :\", error_rate )\n",
    "# the test results are bad because of the wrong implementation of the backpropagation in the outputlayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c54c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
