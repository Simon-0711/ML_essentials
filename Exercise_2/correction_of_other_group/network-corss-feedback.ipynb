{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise Sheet 02\n",
        "\n",
        "- Johannes Sindlinger\n",
        "- Daniel Knorr\n",
        "- Marvin Hanf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5y1KQjW_EXA"
      },
      "source": [
        "# Datenanalyse von MakeMoon Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "H3peVyPn_K86",
        "outputId": "b9f3584e-1214-440e-8d2d-50b64d897bab"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.datasets import make_moons\n",
        " \n",
        "# X are the generated instances, an array of shape (2000,2).\n",
        "# y are the labels of X, with values of either 0 or 1.\n",
        "X, y = make_moons(n_samples=2000, noise=0.05)\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# When the label y is 0, the class is represented with a blue square.\n",
        "# When the label y is 1, the class is represented with a green triangle.\n",
        "\n",
        "# X contains two features, x1 and x2\n",
        "plt.xlabel(r\"$x_1$\", fontsize=20)\n",
        "plt.ylabel(r\"$x_2$\", fontsize=20)\n",
        "# ------------------------------\n",
        "\n",
        "# standardize features to be in [-1, 1]\n",
        "offset  = X.min(axis=0)\n",
        "scaling = X.max(axis=0) - offset\n",
        "X_train = ((X - offset) / scaling - 0.5) * 2.0\n",
        "plt.plot(X_train[:, 0][y==1], X_train[:, 1][y==1], \"bs\")\n",
        "plt.plot(X_train[:, 0][y==0], X_train[:, 1][y==0], \"g^\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd56a1HQ-04G"
      },
      "source": [
        "# 3 Programming a neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "B_LVvDrx8_dL",
        "outputId": "909b8f54-9307-480d-ad8a-901e0886c51a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "\n",
        "\n",
        "# ###################################\n",
        "class ReLULayer(object):\n",
        "\n",
        "    def forward(self, input):\n",
        "        # remember the input for later backpropagation\n",
        "        self.input = input\n",
        "        # return the ReLU of the input\n",
        "        relu = np.maximum(input, 0)  # * your code here\n",
        "        return relu\n",
        "\n",
        "    def backward(self, upstream_gradient):\n",
        "        # compute the derivative of ReLU from upstream_gradient and the stored input\n",
        "        downstream_gradient = upstream_gradient * \\\n",
        "            (self.input > 0)  # * your code here\n",
        "        return downstream_gradient\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        pass  # ReLU is parameter - free\n",
        "\n",
        "\n",
        "# ###################################\n",
        "\n",
        "# ###################################\n",
        "class OutputLayer(object):\n",
        "    def __init__(self, n_classes):\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, input):\n",
        "        # remember the input for later backpropagation\n",
        "        self.input = input\n",
        "        # return the softmax of the input\n",
        "        # 2d array as input, where the softmax function should be applied to each row\n",
        "        # softmax responses probability distribution, where the sum of a row is 1\n",
        "        exp = np.exp(input)\n",
        "        softmax = exp / np.sum(exp, axis=1, keepdims=True)  # * your code here\n",
        "        # print(\"Softmax\", softmax)\n",
        "        return softmax\n",
        "\n",
        "    def backward(self, predicted_posteriors, true_labels):\n",
        "        # return the loss derivative with respect to the stored inputs\n",
        "        # (use cross - entropy loss and the chain rule for softmax ,\n",
        "        # as derived in the lecture )\n",
        "        # compute the ce-loss of (200,1) and (200,2) shape arrays\n",
        "        # --> to do so, return the shape of the predicted_posteriors (otherwise shape error), so we compute the loss\n",
        "        # with the array shapes (2,200) and (200,1)\n",
        "\n",
        "        # cross_entropy_loss = -np.sum(true_labels * np.log2(predicted_posteriors.T))\n",
        "        # print(f\"loss = {cross_entropy_loss}\")\n",
        "\n",
        "        # * your code here\n",
        "        downstream_gradient = predicted_posteriors[:,\n",
        "                                                   true_labels] - true_labels\n",
        "        # print(f\"downstream_gradient = {downstream_gradient}\")\n",
        "        return downstream_gradient\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        pass  # softmax is parameter - free\n",
        "\n",
        "\n",
        "# ###################################\n",
        "\n",
        "# ###################################\n",
        "class LinearLayer(object):\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "        # print(self.n_inputs, self.n_outputs)\n",
        "        # randomly initialize weights and intercepts\n",
        "        # shape Beta --> (2,5) --> (5,5) --> (5,2)\n",
        "        self.B = np.random.normal(loc=0, scale=1, size=(\n",
        "            n_inputs, n_outputs))  # * your code here\n",
        "        # shape bias --> (1,5) --> (1,5) --> (1,2)\n",
        "        self.b = np.random.normal(loc=0, scale=1, size=(\n",
        "            1, n_outputs))  # * your code here\n",
        "\n",
        "    def forward(self, input):\n",
        "        # remember the input for later backpropagation\n",
        "        self.input = input\n",
        "        # print(np.shape(input), self.n_inputs, self.n_outputs)\n",
        "        # compute the scalar product of input and weights\n",
        "        # ( these are the preactivations for the subsequent non-linear layer )\n",
        "        # input has to be in first position of the dot product function, otherwise there will be shape problems\n",
        "        # of course self.B @ input - self.b would be correct\n",
        "        preactivations = np.dot(input, self.B) - self.b  # * your code here\n",
        "        return preactivations\n",
        "\n",
        "    def backward(self, upstream_gradient):\n",
        "        # print(upstream_gradient)\n",
        "        print(np.shape(upstream_gradient), np.shape(self.input))\n",
        "        # compute the derivative of the weights from\n",
        "        # upstream_gradient and the stored input\n",
        "\n",
        "        '''\n",
        "        Here we would do back propagation of the weights and biases. We were not able to implement it, because we have some shape issues we were not able to fix.\n",
        "        For the weights we need the gradient of the total error regarding the weight, so for a specific weight it would be:\n",
        "        Gradient totalError derived by the weight. Using the chain rule and nabla operator it would look something like this:\n",
        "\n",
        "        nabla totalError / nabla weight = nabla totalError / nabla neuronOutput * nabla neuronOutput / nabla neuronInput * nabla neuronInput / nabla weight\n",
        "\n",
        "        This would be self.grad_B for the weights. For the bias the same thing should be done.\n",
        "        Both of these gradients are usable to update the weights and biases for the next learning iteration.\n",
        "        '''\n",
        "        \n",
        "        self.grad_b = ...   # * your code here\n",
        "        self.grad_B = ...  # * your code here\n",
        "\n",
        "        # compute the downstream gradient to be passed to the preceding layer\n",
        "        downstream_gradient = ...  # * your code here\n",
        "        return downstream_gradient\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        # update the weights by batch gradient descent\n",
        "        self.B = self.B - learning_rate * self.grad_B\n",
        "        self.b = self.b - learning_rate * self.grad_b\n",
        "\n",
        "\n",
        "# ###################################\n",
        "\n",
        "# ###################################\n",
        "class MLP(object):\n",
        "    def __init__(self, n_features, layer_sizes):\n",
        "        # constuct a multi - layer perceptron\n",
        "        # with ReLU activation in the hidden layers and softmax output\n",
        "        # (i.e. it predicts the posterior probability of a classification problem )\n",
        "        #\n",
        "        # n_features : number of inputs\n",
        "        # len( layer_size ): number of layers\n",
        "        # layer_size [k]: number of neurons in layer k\n",
        "        # ( specifically : layer_sizes [ -1] is the number of classes )\n",
        "        self.n_layers = len(layer_sizes)\n",
        "        self.layers = []\n",
        "        # create interior layers ( linear + ReLU )\n",
        "        n_in = n_features\n",
        "        for n_out in layer_sizes[: -1]:\n",
        "            self.layers.append(LinearLayer(n_in, n_out))\n",
        "            self.layers.append(ReLULayer())\n",
        "            n_in = n_out\n",
        "        # create last linear layer + output layer\n",
        "        n_out = layer_sizes[-1]\n",
        "        self.layers.append(LinearLayer(n_in, n_out))\n",
        "        self.layers.append(OutputLayer(n_out))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X is a mini - batch of instances\n",
        "        batch_size = X.shape[0]\n",
        "        # flatten the other dimensions of X (in case instances are images )\n",
        "        X = X.reshape(batch_size, -1)\n",
        "        # compute the forward pass\n",
        "        # ( implicitly stores internal activations for later backpropagation )\n",
        "        result = X\n",
        "        for layer in self.layers:\n",
        "            result = layer.forward(result)\n",
        "        return result\n",
        "\n",
        "    def backward(self, predicted_posteriors, true_classes):\n",
        "        # perform backpropagation w.r.t. the prediction for the latest mini - batch X\n",
        "        # * your code here\n",
        "        # backpropagation: enter the layers from the back to the front\n",
        "        # first enter the backward function of the output layer with predicted_posteriors and true_classes\n",
        "        result = self.layers[-1].backward(predicted_posteriors, true_classes)\n",
        "        # then after computing the downstream_gradient in the output layer, enter the layers reversed\n",
        "        # in the layer list (skip the output layer) and pass results through the layers\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            result = layer.backward(result)\n",
        "        return result\n",
        "\n",
        "    def update(self, X, Y, learning_rate):\n",
        "        posteriors = self.forward(X)\n",
        "        # print(f\"posteriors = {posteriors.shape}\")\n",
        "        self.backward(posteriors, Y)\n",
        "        for layer in self.layers:\n",
        "            layer.update(learning_rate)\n",
        "\n",
        "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
        "        N = len(x)\n",
        "        n_batches = N // batch_size\n",
        "        for i in range(n_epochs):\n",
        "            # print (\" Epoch \", i)\n",
        "            # reorder data for every epoch\n",
        "            # (i.e. sample mini - batches without replacement )\n",
        "            permutation = np.random.permutation(N)\n",
        "            for batch in range(n_batches):\n",
        "                # create mini - batch\n",
        "                start = batch * batch_size\n",
        "                x_batch = x[permutation[start: start + batch_size]]\n",
        "                y_batch = y[permutation[start: start + batch_size]]\n",
        "                # perform one forward and backward pass and update network parameters\n",
        "                self.update(x_batch, y_batch, learning_rate)\n",
        "\n",
        "\n",
        "# #################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(200, 200) (200, 5)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for *: 'ellipsis' and 'bool'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Marvin\\Documents\\Uni\\SS23\\ML_Essentials_Exercises\\Exercise2\\network.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m network \u001b[39m=\u001b[39m MLP(n_features, layer_sizes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# train\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m network\u001b[39m.\u001b[39;49mtrain(X_train, Y_train, n_epochs, batch_size, learning_rate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m predicted_posteriors \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39mforward(X_test)\n",
            "\u001b[1;32mc:\\Users\\Marvin\\Documents\\Uni\\SS23\\ML_Essentials_Exercises\\Exercise2\\network.ipynb Cell 6\u001b[0m in \u001b[0;36mMLP.train\u001b[1;34m(self, x, y, n_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m y_batch \u001b[39m=\u001b[39m y[permutation[start: start \u001b[39m+\u001b[39m batch_size]]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m \u001b[39m# perform one forward and backward pass and update network parameters\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(x_batch, y_batch, learning_rate)\n",
            "\u001b[1;32mc:\\Users\\Marvin\\Documents\\Uni\\SS23\\ML_Essentials_Exercises\\Exercise2\\network.ipynb Cell 6\u001b[0m in \u001b[0;36mMLP.update\u001b[1;34m(self, X, Y, learning_rate)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m posteriors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(X)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m \u001b[39m# print(f\"posteriors = {posteriors.shape}\")\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward(posteriors, Y)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m     layer\u001b[39m.\u001b[39mupdate(learning_rate)\n",
            "\u001b[1;32mc:\\Users\\Marvin\\Documents\\Uni\\SS23\\ML_Essentials_Exercises\\Exercise2\\network.ipynb Cell 6\u001b[0m in \u001b[0;36mMLP.backward\u001b[1;34m(self, predicted_posteriors, true_classes)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m \u001b[39m# then after computing the downstream_gradient in the output layer, enter the layers reversed\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m \u001b[39m# in the layer list (skip the output layer) and pass results through the layers\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m     result \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(result)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "\u001b[1;32mc:\\Users\\Marvin\\Documents\\Uni\\SS23\\ML_Essentials_Exercises\\Exercise2\\network.ipynb Cell 6\u001b[0m in \u001b[0;36mReLULayer.backward\u001b[1;34m(self, upstream_gradient)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, upstream_gradient):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# compute the derivative of ReLU from upstream_gradient and the stored input\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     downstream_gradient \u001b[39m=\u001b[39m upstream_gradient \u001b[39m*\u001b[39;49m \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m)  \u001b[39m# * your code here\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marvin/Documents/Uni/SS23/ML_Essentials_Exercises/Exercise2/network.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m downstream_gradient\n",
            "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'ellipsis' and 'bool'"
          ]
        }
      ],
      "source": [
        "# #################################\n",
        "# if __name__ == \" __main__ \":\n",
        "# set training / test set size\n",
        "N = 2000\n",
        "\n",
        "# create training and test data\n",
        "X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
        "X_test, Y_test = datasets.make_moons(N, noise=0.05)\n",
        "n_features = 2\n",
        "n_classes = 2\n",
        "\n",
        "# standardize features to be in [-1, 1]\n",
        "offset = X_train.min(axis=0)\n",
        "scaling = X_train.max(axis=0) - offset\n",
        "X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
        "X_test = ((X_test - offset) / scaling - 0.5) * 2.0\n",
        "# print(f\"Y_train = {Y_train}\")\n",
        "# print(f\"Y_train = {Y_train.shape}\")\n",
        "\n",
        "# set hyperparameters ( play with these !)\n",
        "layer_sizes = [5, 5, n_classes]\n",
        "n_epochs = 5\n",
        "batch_size = 200\n",
        "learning_rate = 0.05\n",
        "\n",
        "# create network\n",
        "network = MLP(n_features, layer_sizes)\n",
        "\n",
        "# train\n",
        "network.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
        "\n",
        "# test\n",
        "predicted_posteriors = network.forward(X_test)\n",
        "\n",
        "# determine class predictions from posteriors by winner -takes -all rule\n",
        "predicted_classes = ...  # * your code here\n",
        "\n",
        "# compute and output the error rate of predicted_classes\n",
        "error_rate = ...  # * your code here\n",
        "print(\" error rate :\", error_rate)\n",
        "\n",
        "# #################################\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
