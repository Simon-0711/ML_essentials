{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "      this class will later get the following attributes\n",
    "      all nodes:\n",
    "          features\n",
    "          responses\n",
    "      split nodes additionally:\n",
    "          left\n",
    "          right\n",
    "          split_index\n",
    "          threshold\n",
    "      leaf nodes additionally\n",
    "          prediction\n",
    "    '''\n",
    "        \n",
    "\n",
    "class Tree:\n",
    "    '''\n",
    "      base class for RegressionTree and ClassificationTree\n",
    "    '''\n",
    "    def __init__(self, n_min=10):\n",
    "        '''n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        self.n_min = n_min \n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' return the prediction for the given 1-D feature vector x\n",
    "        '''\n",
    "        # first find the leaf containing the 1-D feature vector x\n",
    "        node = self.root\n",
    "        while not hasattr(node, \"prediction\"):\n",
    "            j = node.split_index\n",
    "            t = np.array(x)\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        # finally, return the leaf's prediction\n",
    "        return node.prediction\n",
    "        \n",
    "    def train(self, features, responses, D_try=None):\n",
    "        '''\n",
    "        features: the feature matrix of the training set\n",
    "        response: the vector of responses\n",
    "        '''\n",
    "        N, D = features.shape\n",
    "        assert(responses.shape[0] == N)\n",
    "\n",
    "        if D_try is None:\n",
    "            D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "        \n",
    "        # initialize the root node\n",
    "        self.root = Node()\n",
    "        self.root.features  = features\n",
    "        self.root.responses = responses\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            active_indices = self.select_active_indices(D, D_try)\n",
    "            left, right = self.make_split_node(node, active_indices)\n",
    "            if left is None: # no split found\n",
    "                self.make_leaf_node(node)\n",
    "            else:\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "    \n",
    "    def make_split_node(self, node, indices):\n",
    "        '''\n",
    "        node: the node to be split\n",
    "        indices: a numpy array of length 'D_try', containing the feature \n",
    "                         indices to be considered for the present split\n",
    "                         \n",
    "        return: None, None -- if no suitable split has been found, or\n",
    "                left, right -- the children of the split\n",
    "        '''\n",
    "        # all responses equal => no improvement possible by any split\n",
    "        if np.unique(node.responses).shape[0] == 1:\n",
    "            return None, None\n",
    "        \n",
    "        # find best feature j_min (among 'indices') and best threshold t_min for the split\n",
    "        l_min = float('inf')  # upper bound for the loss, later the loss of the best split\n",
    "        j_min, t_min = None, None\n",
    "\n",
    "        for j in indices:\n",
    "            thresholds = self.find_thresholds(node, j)\n",
    "\n",
    "            # compute loss for each threshold\n",
    "            for t in thresholds:\n",
    "                loss = self.compute_loss_for_split(node, j, t)\n",
    "\n",
    "                # remember the best split so far \n",
    "                # (the condition is never True when loss = float('inf') )\n",
    "                if loss < l_min:\n",
    "                    l_min = loss\n",
    "                    j_min = j\n",
    "                    t_min = t\n",
    "\n",
    "        if j_min is None: # no split found\n",
    "            return None, None\n",
    "\n",
    "        # create children for the best split\n",
    "        left, right = self.make_children(node, j_min, t_min)\n",
    "\n",
    "        # turn the current 'node' into a split node\n",
    "        # (store children and split condition)\n",
    "        # ... # your code here\n",
    "        node.left = left\n",
    "        node.right = right\n",
    "        node.split_index = j_min\n",
    "        node.threshold = t_min\n",
    "        # raise NotImplementedError(\"make_split_node(): remove this exception after adding your code above.\")\n",
    "        \n",
    "        # return the children (to be placed on the stack)\n",
    "        return left, right\n",
    "    \n",
    "    def select_active_indices(self, D, D_try):\n",
    "        ''' return a 1-D array with D_try randomly selected indices from 0...(D-1).\n",
    "        '''\n",
    "        sub_list = np.random.choice(D, size=D_try)\n",
    "        return sub_list\n",
    "        #... # your code here\n",
    "        #raise NotImplementedError(\"select_active_indices(): remove this exception after adding your code above.\")\n",
    "        \n",
    "    def find_thresholds(self, node, j):\n",
    "        ''' return: a 1-D array with all possible thresholds along feature j\n",
    "        '''\n",
    "        test = np.array(node.features)\n",
    "        sorted_feature = np.sort(np.unique(node.features[:,j]))\n",
    "        thresholds = []\n",
    "        for i in range(len(sorted_feature) - 1):\n",
    "            threshold = (sorted_feature[i] + sorted_feature[i+1]) / 2  # Compute the midpoint between adjacent elements\n",
    "            thresholds.append(threshold)\n",
    "        #... # your code here\n",
    "        return thresholds\n",
    "        # raise NotImplementedError(\"find_thresholds(): remove this exception after adding your code above.\")\n",
    "        \n",
    "    def make_children(self, node, j, t):\n",
    "        ''' execute the split in feature j at threshold t\n",
    "        \n",
    "            return: left, right -- the children of the split, with features and responses\n",
    "                                   properly assigned according to the split\n",
    "        '''\n",
    "        left = Node()\n",
    "        right = Node()\n",
    "        \n",
    "        left_indices = node.features[:, j] <= t\n",
    "        right_indices = node.features[:, j] > t\n",
    "                \n",
    "        left.features = node.features[left_indices]\n",
    "        right.features = node.features[right_indices]\n",
    "                \n",
    "        left.responses = node.responses[left_indices]\n",
    "        right.responses = node.responses[right_indices]\n",
    "        # ... # your code here\n",
    "        # raise NotImplementedError(\"make_children(): remove this exception after adding your code above.\")\n",
    "        \n",
    "        return left, right\n",
    "        \n",
    "    @abstractmethod\n",
    "    def make_leaf_node(self, node):\n",
    "        ''' Turn node into a leaf by computing and setting `node.prediction`\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"make_leaf_node() must be implemented in a subclass.\")\n",
    "        \n",
    "    @abstractmethod\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        ''' Return the resulting loss when the data are split along feature j at threshold t.\n",
    "            If the split is not admissible, return float('inf').\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"compute_loss_for_split() must be implemented in a subclass.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(Tree):\n",
    "    def __init__(self, n_min=10):\n",
    "        super(RegressionTree, self).__init__(n_min)\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        left_indices = node.features[:, j] <= t\n",
    "        right_indices = node.features[:, j] > t\n",
    "        \n",
    "        if len(left_indices) < self.n_min or len(right_indices) < self.n_min:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        left_responses = node.responses[left_indices]\n",
    "        right_responses = node.responses[right_indices]\n",
    "\n",
    "        left_mean = np.mean(left_responses)\n",
    "        right_mean = np.mean(right_responses)\n",
    "                \n",
    "        left_sse = np.sum((left_responses - left_mean) ** 2)\n",
    "        right_sse = np.sum((right_responses - right_mean) ** 2)\n",
    "        total_sse = left_sse + right_sse\n",
    "\n",
    "        return total_sse\n",
    "        # ... # your code here\n",
    "        # raise NotImplementedError(\"compute_loss_for_split(): remove this exception after adding your code above.\")\n",
    "        \n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a regression tree is a real number)\n",
    "        unique, counts = np.unique(node.responses, return_counts=True)\n",
    "        # Find the index of the element with the maximum count\n",
    "        max_count_index = np.argmax(counts)\n",
    "        # Get the element with the maximum count\n",
    "        predicted_element = unique[max_count_index]\n",
    "        # since it apparently has to be a real number, remask the elements\n",
    "        predicted_element = 3 if predicted_element == 1 else 9\n",
    "        node.prediction = predicted_element \n",
    "        # ... # your code here\n",
    "        # raise NotImplementedError(\"make_leaf_node(): remove this exception after adding your code above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationTree(Tree):\n",
    "    '''implement classification tree so that it can handle arbitrary many classes\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, classes, n_min=10):\n",
    "        ''' classes: a 1-D array with the permitted class labels\n",
    "            n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        super(ClassificationTree, self).__init__(n_min)\n",
    "        self.classes = classes\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # Return the loss if we would split the instances along feature j at threshold t,\n",
    "        # or float('inf') if there is no feasible split.\n",
    "        \n",
    "        left_indices = node.features[:, j] <= t\n",
    "        right_indices = node.features[:, j] > t\n",
    "        \n",
    "        if len(left_indices) < self.n_min or len(right_indices) < self.n_min:\n",
    "            return float('inf')\n",
    "        \n",
    "        left_responses = node.responses[left_indices]\n",
    "        right_responses = node.responses[right_indices]\n",
    "                \n",
    "        # Calculate the Gini impurity for the left and right subsets\n",
    "        left_gini = self.calculate_gini_impurity(left_responses)\n",
    "        right_gini = self.calculate_gini_impurity(right_responses)\n",
    "        total_gini = (len(left_indices)) * left_gini + (len(right_indices)) * right_gini\n",
    "        \n",
    "        return total_gini\n",
    "        # ... # your code here\n",
    "        # raise NotImplementedError(\"compute_loss_for_split(): remove this exception after adding your code above.\")\n",
    "        \n",
    "    def calculate_gini_impurity(self, responses):\n",
    "        # Calculate the Gini impurity for each subnode and all classes\n",
    "        \n",
    "        _, counts = np.unique(responses, return_counts=True)\n",
    "        probabilities = counts / len(responses)\n",
    "        gini_impurity = 1.0 - np.sum(probabilities ** 2)\n",
    "        \n",
    "        return gini_impurity\n",
    "        \n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a classification tree is a class label)\n",
    "        unique_classes, counts = np.unique(node.responses, return_counts=True)\n",
    "\n",
    "        # Filter the unique classes based on the permitted classes\n",
    "        filtered_classes = np.intersect1d(unique_classes, self.classes)\n",
    "\n",
    "        if len(filtered_classes) > 0:\n",
    "            predicted_class_index = np.argmax(counts[unique_classes == filtered_classes])\n",
    "            predicted_class = filtered_classes[predicted_class_index]\n",
    "        else:\n",
    "            # If no permitted class is present in the node's responses, select the most frequent class\n",
    "            # no other useful use of self.classes came into my mind...\n",
    "            predicted_class_index = np.argmax(counts)\n",
    "            predicted_class = unique_classes[predicted_class_index]\n",
    "\n",
    "        node.prediction = predicted_class\n",
    "        # ... # your code here\n",
    "        # raise NotImplementedError(\"make_leaf_node(): remove this exception after adding your code above.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data and extract 3s and 9s\n",
    "digits = load_digits()\n",
    "print(digits.data.shape, digits.target.shape)\n",
    "\n",
    "instances = (digits.target == 3) | (digits.target == 9)\n",
    "features = digits.data[instances, :]\n",
    "labels = digits.target[instances]\n",
    "\n",
    "# for regression, we use labels +1 and -1\n",
    "responses = np.array([1 if l == 3 else -1 for l in labels])\n",
    "\n",
    "assert(features.shape[0] == labels.shape[0] == responses.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean error rate for training with n_min of 5: 0.0967171717171717, standard deviation: 0.039928014418550185\n",
      "The mean error rate for training with n_min of 10: 0.1287037037037037, standard deviation: 0.04576547539354592\n",
      "The mean error rate for training with n_min of 20: 0.14267676767676768, standard deviation: 0.0570453123282618\n",
      "The mean error rate for training with n_min of 50: 0.17112794612794613, standard deviation: 0.1011246194268189\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionTree()\n",
    "# and comment on your results\n",
    "# ... # your code here\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "ratio = 0.75\n",
    "length = features.shape[0]\n",
    "idx = round(ratio*length)\n",
    "\n",
    "X_train, y_train = features[:idx], responses[:idx]\n",
    "X_test, y_test = features[idx:], responses[idx:]\n",
    "\n",
    "\n",
    "def regression_cross_validation(X_train, y_train, n_mins):\n",
    "    for n_min in n_mins:\n",
    "        n_folds = 5\n",
    "        k_folds = KFold(n_splits=n_folds)\n",
    "        mean_rate = np.zeros(len(n_mins))\n",
    "        for i, (train, val) in enumerate(k_folds.split(X_train, y_train)):\n",
    "            train_X, train_y = features[train], responses[train]\n",
    "            X_val, y_val = features[val], responses[val]\n",
    "            RegTree = RegressionTree(n_min=n_min)\n",
    "            RegTree.train(train_X, train_y)\n",
    "            y_preds = []\n",
    "            for sample in range(X_val.shape[0]):\n",
    "                y_preds.append(RegTree.predict(X_val[sample]))\n",
    "            # remask the ground truth responses, as the regression tree shall produce real numbers as predictions\n",
    "            y_val = np.array([3 if y == 1 else 9 for y in y_val])\n",
    "            mean_rate[i-1] = np.mean(y_preds != y_val)\n",
    "        print(f\"The mean error rate for training with n_min of {n_min}: {np.mean(mean_rate)}, standard deviation: {np.std(mean_rate)}\")\n",
    "\n",
    "regression_cross_validation(X_train, y_train, [5, 10, 20, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best result for n_min of 5, overall the error rate is decent but not as low as I wished for. Compared to the sample solution in ex1 for the same task with svm's, this has a similar error rate. Trainig time is very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test error with n_min of 5: 0.13186813186813187\n"
     ]
    }
   ],
   "source": [
    "n_min = [5]\n",
    "RegTree = RegressionTree(n_min[0])\n",
    "RegTree.train(X_train, y_train, n_min)\n",
    "y_preds = []\n",
    "for sample in range(X_test.shape[0]):\n",
    "    y_preds.append(RegTree.predict(X_test[sample]))\n",
    "y_test = np.array([3 if y == 1 else 9 for y in y_test])\n",
    "print(f\"The test error with n_min of {n_min[0]}: {np.mean(y_preds != y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean error rate for training with n_min of 5: 0.152020202020202, standard deviation: 0.02365125146256181\n",
      "The mean error rate for training with n_min of 10: 0.12937710437710437, standard deviation: 0.0476096984690765\n",
      "The mean error rate for training with n_min of 20: 0.06927609427609427, standard deviation: 0.038039967591057385\n",
      "The mean error rate for training with n_min of 50: 0.1516835016835017, standard deviation: 0.04947118007320392\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using ClassificationTree(classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "\n",
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionTree()\n",
    "# and comment on your results\n",
    "# ... # your code here\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "ratio = 0.75\n",
    "length = features.shape[0]\n",
    "idx = round(ratio*length)\n",
    "classes = np.unique(labels)\n",
    "\n",
    "X_train, y_train = features[:idx], labels[:idx]\n",
    "X_test, y_test = features[idx:], labels[idx:]\n",
    "\n",
    "\n",
    "def classification_cross_validation(X_train, y_train, n_mins):\n",
    "    for n_min in n_mins:\n",
    "        n_folds = 5\n",
    "        k_folds = KFold(n_splits=n_folds)\n",
    "        mean_rate = np.zeros(len(n_mins))\n",
    "        for i, (train, val) in enumerate(k_folds.split(X_train, y_train)):\n",
    "            train_X, train_y = features[train], labels[train]\n",
    "            X_val, y_val = features[val], labels[val]\n",
    "            ClassTree = ClassificationTree(n_min=n_min, classes=classes)\n",
    "            ClassTree.train(train_X, train_y)\n",
    "            y_preds = []\n",
    "            for sample in range(X_val.shape[0]):\n",
    "                y_preds.append(ClassTree.predict(X_val[sample]))\n",
    "            mean_rate[i-1] = np.mean(y_preds != y_val)\n",
    "        print(f\"The mean error rate for training with n_min of {n_min}: {np.mean(mean_rate)}, standard deviation: {np.std(mean_rate)}\")\n",
    "\n",
    "classification_cross_validation(X_train, y_train, [5, 10, 20, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best result for n_min = 20. Decent error rate especially for n_min = 20. The training time is a bit longer than for the regressiontree, which might be due to my implementation or due to the fact that it took more calculation steps. But still it was a fast training time with < 10 seconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test error with n_min of 20: 0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "n_min = [20]\n",
    "ClassTree = ClassificationTree(n_min=n_min[0], classes = classes)\n",
    "ClassTree.train(X_train, y_train, n_min)\n",
    "y_preds = []\n",
    "for sample in range(X_test.shape[0]):\n",
    "    y_preds.append(ClassTree.predict(X_test[sample]))\n",
    "# y_test = np.array([3 if y == 1 else 9 for y in y_test])\n",
    "print(f\"The test error with n_min of {n_min[0]}: {np.mean(y_preds != y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sampling(features, responses):\n",
    "    '''return a bootstrap sample of features and responses\n",
    "    '''\n",
    "\n",
    "    # length of dataset\n",
    "    N = responses.shape[0]\n",
    "    indices = np.random.randint(low=0, high=N, size=N)\n",
    "    # features and responses can be selected more than once \n",
    "    return features[indices], responses[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "bootstrap_features, bootstrap_responses = bootstrap_sampling(digits.data, digits.target)\n",
    "\n",
    "print(bootstrap_features.shape, bootstrap_responses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegressionForest():\n",
    "    def __init__(self, n_trees, n_min=10):\n",
    "        # create ensemble\n",
    "        self.trees = [RegressionTree(n_min) for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, features, responses):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, responses)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        responses = np.array([tree.predict(x=x) for tree in self.trees])\n",
    "        # ensemble response regression: average over all tree responses\n",
    "        return np.mean(responses)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationForest():\n",
    "    def __init__(self, n_trees, classes, n_min=1):\n",
    "        self.trees = [ClassificationTree(classes, n_min) for i in range(n_trees)]\n",
    "        self.classes = classes\n",
    "    \n",
    "    def train(self, features, labels):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, responses)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        responses = np.array([tree.predict(x=x) for tree in self.trees])\n",
    "        # count response for each unique class\n",
    "        unique, counts = np.unique(responses, return_counts=True)\n",
    "        # return class with most counts\n",
    "        return unique[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionForest(n_trees=10)\n",
    "# and comment on your results\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using DecisionForest(n_trees=10, classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DecisionForest(n_trees=10, classes=np.unique(digits.target))\n",
    "# for all 10 digits simultaneously.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "... # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-against-the-rest classification with RegressionForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ten one-against-the-rest regression forests for the 10 digits.\n",
    "# Make sure that all training sets are balanced between the current digit and the rest.\n",
    "# Assign test instances to the digit with highest score, \n",
    "# or to \"unknown\" if all scores are negative.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "... # your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e58a8ccaa852cc08331f37e17949d170386811f8cc6b88ac3f9ac5095eeb8162"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
